{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"fednlp.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"pycharm":{"stem_cell":{"cell_type":"raw","metadata":{"collapsed":false},"source":[]}}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QvLPcQ6HC6ii"},"source":["# Investigate impacts from the wording of Federal Reserve (Fed) meetings to financial market.\n","\n","The objective of this project is to apply NLP on those texts published by FOMC to attempt to\n","quantify the communications in a systematic manner and find the relationship with Treasury\n","yield curve."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9z_x7-aaC6ij"},"source":["# Table of Contents\n","\n","\n","1. [Data Overview](#data-o)\n","    <br>1.1 [Summary](#sum)\n","    <br>1.2 [Sections](#t-sect)\n","    <br>1.3 [Crop Sections](#crop)\n","\n","2. [Data Pre-processing](#data-p)\n","    <br>2.1 [Tokenization](#stemlem)\n","\n","3. [Topic Modelling](#topic-m)\n","    <br>3.1 [Non-negative Matrix Factorization](#nmf)\n","    <br>3.2 [Latent Dirichlet Allocation](#lda)\n"," \n","4. [Sentiment Analysis](#sent-a)\n","    <br>4.1 [Dictionary Approach](#dict)\n","    <br>4.2 [Sentiment Extraction](#sent-e)\n","    \n","5. [Modeling](#model)\n","    <br>5.1 [Sentiment Correlations and Baseline Model](#base)\n","    <br>5.2 [BERT and CNN Model](#bert)\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cH7hTfI_uBdb","colab":{},"executionInfo":{"status":"ok","timestamp":1596243659484,"user_tz":240,"elapsed":1018,"user":{"displayName":"Aaron Cheng","photoUrl":"","userId":"04040563564242793771"}}},"source":["file_path_prefix = None"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9mXxv7ZWC8yc","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596243795828,"user_tz":240,"elapsed":137354,"user":{"displayName":"Aaron Cheng","photoUrl":"","userId":"04040563564242793771"}},"outputId":"32abb9eb-96c3-4842-dcf2-9fa0d5397ff8"},"source":["#if using colab\n","from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install -r drive/My\\ Drive\\/fednlp/requirements.txt\n","file_path_prefix = './drive/My Drive/fednlp/data/'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: bs4==0.0.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/fednlp/requirements.txt (line 1)) (0.0.1)\n","Collecting torch==1.5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/01/457b49d790b6c4b9720e6f9dbbb617692f6ce8afdaadf425c055c41a7416/torch-1.5.1-cp36-cp36m-manylinux1_x86_64.whl (753.2MB)\n","\u001b[K     |████████████████████████████████| 753.2MB 22kB/s \n","\u001b[?25hCollecting torchvision==0.6.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/f1/535a407b4a265adf2dd7c2c2458217e37c5fe83ec97234e66c564592a9a0/torchvision-0.6.1-cp36-cp36m-manylinux1_x86_64.whl (6.6MB)\n","\u001b[K     |████████████████████████████████| 6.6MB 20.3MB/s \n","\u001b[?25hRequirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/fednlp/requirements.txt (line 4)) (1.0.0)\n","Requirement already satisfied: seaborn==0.10.1 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/fednlp/requirements.txt (line 5)) (0.10.1)\n","Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/fednlp/requirements.txt (line 6)) (0.0)\n","Collecting scipy==1.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/20/d4410683e4d416a11ebc60138f6d925f571ffcfcc3794baf78fff982c98d/scipy-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n","\u001b[K     |████████████████████████████████| 25.9MB 116kB/s \n","\u001b[?25hCollecting spacy==2.3.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/80/c3c0d15cc3ea97c1fd578c39489ef6c360ec0fedfbf15cb29fd89dcf3271/spacy-2.3.1-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n","\u001b[K     |████████████████████████████████| 9.9MB 60.7MB/s \n","\u001b[?25hCollecting wordcloud==1.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/e7/f4ed7fac1993615b4ba92f473a77e27a3d210c5d23a000c2c98846963f9a/wordcloud-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (364kB)\n","\u001b[K     |████████████████████████████████| 368kB 57.0MB/s \n","\u001b[?25hRequirement already satisfied: pandas==1.0.5 in /usr/local/lib/python3.6/dist-packages (from -r drive/My Drive/fednlp/requirements.txt (line 10)) (1.0.5)\n","Collecting nltk==3.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n","\u001b[K     |████████████████████████████████| 1.4MB 53.7MB/s \n","\u001b[?25hCollecting requests==2.24.0\n","\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl\u001b[0m\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n","\u001b[K     |████████████████████████████████| 71kB 198kB/s \n","\u001b[?25hCollecting transformers==3.0.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n","\u001b[K     |████████████████████████████████| 778kB 6.6MB/s \n","\u001b[?25hCollecting beautifulsoup4==4.9.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/25/ff030e2437265616a1e9b25ccc864e0371a0bc3adb7c5a404fd661c6f4f6/beautifulsoup4-4.9.1-py3-none-any.whl (115kB)\n","\u001b[K     |████████████████████████████████| 122kB 10.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->-r drive/My Drive/fednlp/requirements.txt (line 2)) (1.18.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1->-r drive/My Drive/fednlp/requirements.txt (line 2)) (0.16.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.6.1->-r drive/My Drive/fednlp/requirements.txt (line 3)) (7.0.0)\n","Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (5.3.1)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (4.7.5)\n","Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (7.5.1)\n","Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (5.6.1)\n","Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (5.2.0)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (4.10.1)\n","Requirement already satisfied: matplotlib>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.10.1->-r drive/My Drive/fednlp/requirements.txt (line 5)) (3.2.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn==0.0->-r drive/My Drive/fednlp/requirements.txt (line 6)) (0.22.2.post1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (3.0.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (1.0.2)\n","Collecting thinc==7.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 13.1MB/s \n","\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (4.41.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (49.2.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (1.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (2.0.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (1.0.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (0.7.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (1.0.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.5->-r drive/My Drive/fednlp/requirements.txt (line 10)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas==1.0.5->-r drive/My Drive/fednlp/requirements.txt (line 10)) (2.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk==3.5->-r drive/My Drive/fednlp/requirements.txt (line 11)) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk==3.5->-r drive/My Drive/fednlp/requirements.txt (line 11)) (0.16.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk==3.5->-r drive/My Drive/fednlp/requirements.txt (line 11)) (2019.12.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.24.0->-r drive/My Drive/fednlp/requirements.txt (line 12)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.24.0->-r drive/My Drive/fednlp/requirements.txt (line 12)) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.24.0->-r drive/My Drive/fednlp/requirements.txt (line 12)) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.24.0->-r drive/My Drive/fednlp/requirements.txt (line 12)) (1.24.3)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r drive/My Drive/fednlp/requirements.txt (line 13)) (0.7)\n","Collecting tokenizers==0.8.1.rc1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 24.9MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r drive/My Drive/fednlp/requirements.txt (line 13)) (20.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->-r drive/My Drive/fednlp/requirements.txt (line 13)) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 44.5MB/s \n","\u001b[?25hCollecting sentencepiece!=0.1.92\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 64.3MB/s \n","\u001b[?25hCollecting soupsieve>1.2\n","  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\n","Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (4.3.3)\n","Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (0.2.0)\n","Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (5.1.1)\n","Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (4.6.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (2.11.2)\n","Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (5.3.5)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (1.5.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (5.0.7)\n","Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (0.8.3)\n","Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (1.9.0)\n","Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (19.0.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (2.1.3)\n","Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (5.5.0)\n","Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (3.5.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (3.1.5)\n","Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (0.4.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (1.4.2)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (0.6.0)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (0.8.4)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (0.3)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (1.0.18)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn==0.10.1->-r drive/My Drive/fednlp/requirements.txt (line 5)) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn==0.10.1->-r drive/My Drive/fednlp/requirements.txt (line 5)) (1.2.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.1.2->seaborn==0.10.1->-r drive/My Drive/fednlp/requirements.txt (line 5)) (2.4.7)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (1.7.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas==1.0.5->-r drive/My Drive/fednlp/requirements.txt (line 10)) (1.15.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (4.4.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (1.1.1)\n","Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (2.6.0)\n","Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (0.6.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (0.7.5)\n","Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (4.8.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (0.8.1)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (0.5.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter==1.0.0->-r drive/My Drive/fednlp/requirements.txt (line 4)) (0.2.5)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.3.1->-r drive/My Drive/fednlp/requirements.txt (line 8)) (3.1.0)\n","Building wheels for collected packages: nltk, sacremoses\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434676 sha256=33346e77d57cb6e1d4dc423e1c2c91ec61bbd0166972b26bd4c6cb5a32329df9\n","  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=0a1564d2fafbaf81293d0b4669323c90213836bfaa666389e8851b5668cdfa99\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built nltk sacremoses\n","\u001b[31mERROR: tensorflow 2.2.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.5.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: torch, torchvision, scipy, requests, thinc, spacy, wordcloud, nltk, tokenizers, sacremoses, sentencepiece, transformers, soupsieve, beautifulsoup4\n","  Found existing installation: torch 1.6.0+cu101\n","    Uninstalling torch-1.6.0+cu101:\n","      Successfully uninstalled torch-1.6.0+cu101\n","  Found existing installation: torchvision 0.7.0+cu101\n","    Uninstalling torchvision-0.7.0+cu101:\n","      Successfully uninstalled torchvision-0.7.0+cu101\n","  Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","  Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","  Found existing installation: wordcloud 1.5.0\n","    Uninstalling wordcloud-1.5.0:\n","      Successfully uninstalled wordcloud-1.5.0\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","  Found existing installation: beautifulsoup4 4.6.3\n","    Uninstalling beautifulsoup4-4.6.3:\n","      Successfully uninstalled beautifulsoup4-4.6.3\n","Successfully installed beautifulsoup4-4.9.1 nltk-3.5 requests-2.24.0 sacremoses-0.0.43 scipy-1.5.0 sentencepiece-0.1.91 soupsieve-2.0.1 spacy-2.3.1 thinc-7.4.1 tokenizers-0.8.1rc1 torch-1.5.1 torchvision-0.6.1 transformers-3.0.2 wordcloud-1.7.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["requests"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_YDJXXlwC6il","colab":{"base_uri":"https://localhost:8080/","height":649},"executionInfo":{"status":"ok","timestamp":1596243821479,"user_tz":240,"elapsed":162999,"user":{"displayName":"Aaron Cheng","photoUrl":"","userId":"04040563564242793771"}},"outputId":"8571910d-0a18-43b3-f183-bb4691137514"},"source":["import re\n","import os,sys\n","import calendar\n","import datetime\n","from datetime import timedelta\n","from collections import Counter\n","import pickle\n","\n","import numpy as np\n","import pandas as pd\n","import scipy\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.decomposition import NMF, LatentDirichletAllocation\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.preprocessing import normalize\n","from sklearn.metrics import accuracy_score\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","\n","import nltk\n","from nltk.corpus import CategorizedPlaintextCorpusReader\n","from wordcloud import WordCloud\n","import spacy\n","from spacy.lang.en import English\n","from transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer\n","\n","\n","if file_path_prefix is None:\n","    file_path_prefix = '/Users/aaroncgw/Google Drive/fednlp/data/'\n","\n","nltk.download('punkt')\n","\n","!python -m spacy download en_core_web_md\n","import en_core_web_md\n","nlp = en_core_web_md.load()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stderr"},{"output_type":"stream","text":["Collecting en_core_web_md==2.3.1\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.1/en_core_web_md-2.3.1.tar.gz (50.8MB)\n","\u001b[K     |████████████████████████████████| 50.8MB 2.4MB/s \n","\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_md==2.3.1) (2.3.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.7.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.41.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.18.5)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.1.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.4.1)\n","Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (7.4.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.24.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (49.2.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.3)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.7.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.24.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.1.0)\n","Building wheels for collected packages: en-core-web-md\n","  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-md: filename=en_core_web_md-2.3.1-cp36-none-any.whl size=50916643 sha256=4fbcf235e60825a743272d89f352677ab3f7a1c697efb9424547d4d5393da4c2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-m8u4lgp2/wheels/6e/65/3a/34cdc26d4084d1d1f1e2ec9914964759ea17aa382c53a57d9f\n","Successfully built en-core-web-md\n","Installing collected packages: en-core-web-md\n","Successfully installed en-core-web-md-2.3.1\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_md')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XiFImql3C6io"},"source":["# <a name=\"data-o\"></a> Data Overview\n","Data was web scrapped from https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm.\n","The year 2004 was chosen as the start-year given the acceleration of release dates to 3 weeks, and improved clarity in explanations of committee’s decisions and views [(Danker,2005)](https://www.federalreserve.gov/pubs/bulletin/2005/spring05_fomc.pdf)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9-nsyQbqC6io"},"source":["## <a name=\"sum\"></a>Minutes Summary"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WYj4ugZ-C6ip","pycharm":{"is_executing":false},"colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"512beff1-3cd8-4708-d456-de0374df249c"},"source":["corpus_root = file_path_prefix + 'minutes'\n","data_m = CategorizedPlaintextCorpusReader(corpus_root, r'.*\\.txt', cat_pattern=r'(\\w+)/*')\n","data_fileids = data_m.fileids()\n","\n","print('Total number of files: '+str(len(data_m.fileids())))\n","print('Number of paragraphs: '+str(len(data_m.paras())))\n","print('Number of sentences: '+str(len(data_m.sents())))\n","print('Number of words: '+str(len(data_m.words())))\n","print('\\n'+'First file: '+ data_fileids[0])\n","print('Last file: '+ data_fileids[-1])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total number of files: 130\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gf3newRkC6ir","pycharm":{"is_executing":false},"colab":{}},"source":["num_para_py = {}\n","num_word_py = {}\n","\n","for y in range(2004,2021):\n","    files = data_m.fileids(str(y))\n","    files_size = len(files)\n","    num_para_py[y] = sum([len(data_m.paras(f))for f in files])/files_size\n","    num_word_py[y] = sum([len(data_m.words(f))for f in files])/files_size\n","        \n","para_words = pd.DataFrame([num_para_py,num_word_py],\n","                          index = ['Average number of paragraphs','Average number of words']).T\n","\n","para_words = para_words.reset_index()\n","\n","fig,ax = plt.subplots(2, 1, figsize=(10,10))\n","sns.barplot(x=\"index\", y=\"Average number of paragraphs\", data=para_words, ax=ax[0])\n","sns.barplot(x=\"index\", y=\"Average number of words\", data=para_words, ax=ax[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"V5nnDJS4C6it"},"source":["We can see an acceleration in the amount of paragraphs and words overtime."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CHzertjEC6it"},"source":["## <a name=\"t-sect\"></a> Sections\n","The minutes are split into 4 sections (see [Background on FOMC Meeting Minutes](https://www.federalreserve.gov/pubs/bulletin/2005/spring05_fomc.pdf)) \n","\n","1. Introduction\n","2. Economic and financial information\n","3. Participants’ views on developments \n","4. Policy decisions\n","\n","The first set of introductory paragraphs contain a list of attendees and procedural items, which are not of use to our analysis and hence need to be removed. The next problem to tackle is knowing when to start the minutes from."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"73XZka1zC6iu","pycharm":{"is_executing":false},"colab":{}},"source":["df_temp = pd.DataFrame()\n","df_year = pd.DataFrame()\n","for y in range(2004,2019):\n","    files = data_m.fileids(str(y))\n","    for f in files:\n","        word_para = [sum([len(s)for s in p]) for p in data_m.paras(f)]\n","        df_temp = pd.concat([df_temp,pd.Series(word_para,name=f)],axis=1)\n","    \n","    df_mean = df_temp.mean(axis=1)\n","    df_mean.name = y\n","    df_year= pd.concat([df_year,df_mean],axis=1)\n","    df_temp = df_temp.iloc[0:0]\n","\n","window_size = 10\n","\n","y1 = df_year.loc[:,[y for y in range(2004,2008)]].mean(axis=1).rolling(window_size).mean()\n","y2 = df_year.loc[:,[y for y in range(2009,2012)]].mean(axis=1).rolling(window_size).mean()\n","y3 = df_year.loc[:,[y for y in range(2012,2019)]].mean(axis=1).rolling(window_size).mean()\n","\n","df_joint = pd.DataFrame([y1,y2,y3],index = ['2004-2007','2008-2011','2012-2018']).T\n","\n","\n","ax = df_joint.plot(figsize=(10,5))\n","ax.set_xlabel(\"Paragraph index\")\n","ax.set_ylabel(\"Word count\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CI6CSt1nC6iw"},"source":["To see how the mintues are structured, we can plot a moving average of words per paragraph. The peaks and troughs of this moving average are indicative of new sections, where usually the start of the section includes a header or a very few words. \n","\n","For simplicity, I've identified three different types of structures that span from 2004-2007, 2008-2011 and 2012-2018. It is quite clear that the early minutes (2004-2007) began it's second section by paragraph 30-40, whilst those minutes later on began around paragraph 50 onwards.\n","  "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NPX3xocwC6iw"},"source":["## <a name=\"crop\"></a>Crop Sections"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u1moodwjuBdr"},"source":["Given that we know where sections roughly start, the easiest way to split our sections is to find the most common words. For our analysis, the minutes will start with phrases similar to 'Staff Review of the Economic Situation' or 'The information reviewed' and end on 'At the conclusion of this meeting' or 'The Committee voted to authorize'."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5wDJa1zqC6iz","colab":{}},"source":["corpus_root_cropped = file_path_prefix + 'minutes_cropped'\n","data_c = CategorizedPlaintextCorpusReader(corpus_root_cropped, r'.*\\.txt', cat_pattern=r'(\\w+)/*')\n","\n","print('Total number of files: '+str(len(data_c.fileids())))\n","print('Number of paragraphs: '+str(len(data_c.paras())))\n","print('Number of sentences: '+str(len(data_c.sents())))\n","print('Number of words: '+str(len(data_c.words())))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2xywoN6hC6i1"},"source":["Total number of paragraphs, sentences and words have been reduced by 30-60%."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cALNXe2OuBdt"},"source":["## <a name=\"rates\"></a>Treasury Rates"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ES4DNJfduBdu","colab":{}},"source":["with open(file_path_prefix + 'rates/rates_daily.pickle', 'rb') as handle:\n","    rates_daily = pickle.load(handle)\n","    \n","with open(file_path_prefix + 'rates/rates_monthly.pickle', 'rb') as handle:\n","    rates_monthly = pickle.load(handle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YKNmMZCzuBdv","colab":{}},"source":["df_yc_slope_changes = pd.DataFrame((rates_monthly['BC_10YEAR'] - rates_monthly['BC_3MONTH']), columns=['slope_change']).diff().shift(-1)[:-1]\n","df_yc_slope_changes['steepen'] = df_yc_slope_changes['slope_change'] > 0\n","df_yc_slope_changes\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BYMLj2tGC6iw"},"source":["# <a name=\"data-p\"></a> Data Pre-processing\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tP-t3rDVC6i1"},"source":["## <a name=\"stemlem\"></a> Tokenization\n","\n","To enhance topic modelling output, we can remove stop words and reduce inflectional forms of words back to its roots - using techniques such as stemming and lemmatization. \n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZOPX2NI1C6i2","colab":{}},"source":["en = English()\n","\n","def simple_tokenizer(doc, model=en):\n","    tokenized_docs = []\n","    parsed = model(doc)\n","    return([t.lemma_.lower() for t in parsed if (t.is_alpha)&(not t.like_url)&(not t.is_stop)])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8tLTouMjuBd0","colab":{}},"source":["def last_day_of_month(any_day):\n","    next_month = any_day.replace(day=28) + datetime.timedelta(days=4)  # this will never fail\n","    return next_month - datetime.timedelta(days=next_month.day)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HRA4M-1juBd2","colab":{}},"source":["idx = []\n","paragraph_list = []\n","for f in data_c.fileids():\n","    year,month,day = re.search(\"(\\d{4})(\\d{2})(\\d{2})\",f).groups()\n","    paragraph_list_single_file = data_c.raw(f).split(\"\\n\\n\")\n","    for i, paragraph in enumerate(paragraph_list_single_file):\n","        p_tokenized = simple_tokenizer(paragraph)\n","        if len(p_tokenized) != 0:\n","            idx.append((last_day_of_month(datetime.datetime(int(year), int(month), int(day))), i)) #Year,Month,Day,Paragraph number\n","            paragraph_list.append([p_tokenized,paragraph])\n","        \n","df_paragraphs = pd.DataFrame(data = paragraph_list,\n","                      index = pd.MultiIndex.from_tuples(idx), \n","                      columns = ['tokenized_text','raw_text'])\n","\n","print(len(df_paragraphs))\n","df_paragraphs.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dIyLp2bPC6i3","colab":{}},"source":["'''\n","idx = []\n","para_number = []\n","paragraph_list = []\n","for f in data_c.fileids():\n","    year,month,day = re.search(\"(\\d{4})(\\d{2})(\\d{2})\",f).groups()\n","    for i,p in enumerate(data_c.paras(f)):\n","        idx.append((last_day_of_month(datetime.datetime(int(year), int(month), int(day))), i)) #Year,Month,Day,Paragraph number\n","        flat_p = [item for sublist in p for item in sublist]\n","        paragraph = ' '.join(flat_p)\n","        p_tokenized = simple_tokenizer(paragraph)\n","        paragraph_list.append([p_tokenized,flat_p])\n","\n","df_paragraphs = pd.DataFrame(data = paragraph_list,\n","                          index = pd.MultiIndex.from_tuples(idx), \n","                          columns = ['tokenized_text','raw_text'])\n","\n","df_paragraphs.head()'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pbIGp5OUC6i5","colab":{}},"source":["#Most frequent words in corpus\n","tokenized_text = df_paragraphs['tokenized_text']\n","all_tokens = [item for sublist in tokenized_text for item in sublist]\n","\n","wordcloud = WordCloud(collocations=False,\n","                      background_color='white',\n","                      mode='RGB',\n","                      width=1600, height=800).generate((\" \").join(all_tokens))\n","\n","fig = plt.figure(figsize=(16,8),dpi=200)\n","plt.axis('off')\n","plt.imshow(wordcloud, interpolation=\"bilinear\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UJp7yA06C6i7"},"source":["# <a name=\"topic-m\"></a>Topic Modelling \n","\n","Now that we have our clean text corpus, we can create a bag of words and pass it into a Latent Dirichlet Allocation model (LDA). This probabilistic model is ideal for our analysis as each paragraph is distributed amongst a set of topics, where topics themselves are defined by words that frequently appear with each other. \n","\n","<br>\n","In our previous example, the phrase \"Labor market conditions continued to strengthen in recent months, with the unemployment rate declining further and payroll gains well above a pace consistent with maintaining a stable unemployment rate over time\" is one that belongs in the labour market category. Additionally, one might say words like labour, payroll and unemployment are more likely to appear in the same document (or in this case paragraph). "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_Zp90hwtC6i8","colab":{}},"source":["def summarize_topics(model, feature_names, no_top_words):\n","    topics = pd.DataFrame()\n","    for topic_idx, topic in enumerate(model.components_):\n","        top_words = [feature_names[i]for i in topic.argsort()[:-no_top_words - 1:-1]]\n","        topics[\"Topic %d:\" % (topic_idx)] = top_words\n","    return topics\n","\n","num_topics = 6"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FdgDvxzCC6jA"},"source":["## <a name=\"lda\"></a>LDA\n","\n","One of the most important inputs in the LDA model is the number of topics (n_components). Given that this is an unsupervised task, there is no best way to choose this input other than trial and error - which in this case results to n_components = 6. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"03zMcH6kC6jA","colab":{}},"source":["count_vecs = CountVectorizer(tokenizer = lambda x: x, lowercase = False)\n","count_vecs_fitted = count_vecs.fit_transform(list(tokenized_text))\n","\n","lda = LatentDirichletAllocation(n_components = num_topics, random_state=7).fit(count_vecs_fitted)\n","lda_t = lda.transform(count_vecs_fitted)\n","\n","df_paragraphs['topic_weights'] = list(lda_t)\n","\n","lda_topic_top_words = summarize_topics(lda,count_vecs.get_feature_names(),10)\n","lda_topic_top_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"E8FuxwtJC6jC"},"source":["The output above shows the top 10 words associated to each topic. Some topics are more distinguishable than others, where words such as inflation, price, energy are usually associated to the topic 'Inflation', whilst others are less so."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5wZHXA-SC6jD","colab":{}},"source":["#Dictionary with topics\n","topic_dict = {0 : 'Inflation',\n","              1 : 'Policy',\n","              2 : 'Investment', \n","              3 : 'Financial Market',\n","              4 : 'Labor Market', \n","              5 : 'Outlook'}\n","topic_dict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"N0EQHqOZC6jE","colab":{}},"source":["def sum_prop(x):\n","    x_sum = x.sum(axis=0)\n","    return (x_sum/x_sum.sum())*100\n","\n","df_topic_t = df_paragraphs.groupby(level=[0],sort=False)['topic_weights']\\\n","                        .apply(lambda x: sum_prop(x)).apply(pd.Series)\\\n","                        .rename(columns = topic_dict)\n","\n","df_topic_m = df_topic_t.groupby(level=[0],sort=False).apply(lambda x: sum_prop(x))\n","topic_perc = df_topic_m.divide(df_topic_m.sum(axis=1), axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Nri5_sm6C6jH","colab":{}},"source":["df_topic_m.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NIsPBViHC6jK","colab":{}},"source":["fig, ax = plt.subplots(figsize=(15,8))\n","ax.stackplot(df_topic_m.index,  topic_perc[\"Labor Market\"],  topic_perc[\"Policy\"],  topic_perc[\"Financial Market\"], \n","              topic_perc[\"Inflation\"],  topic_perc[\"Outlook\"],  topic_perc[\"Investment\"],\n","              labels=['Labor Market','Policy','Financial Market', 'Inflation', 'Outlook', 'Investment'])\n","ax.legend(loc='upper left')\n","ax.margins(0,0)\n","ax.set_title('Topic Percentage Over Time')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uWptveCKC6jL"},"source":["As expected the topic 'Financial markets' increased in proportion after 2008. To date, the largest topic is 'Policy' and the smallest is 'Labor Market'. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TDSTv_0WC6jM"},"source":["# <a name=\"sent-a\"></a> Sentiment Analysis \n","\n","Two sets of dictionaries were used in this section: \n","1. Havard IV-4 Psychosociological \n","2. Loughran and McDonald\n","\n","The former is less tailored to financial statements, whilst Loughran and McDonald is adapted to include words from 10-K documents.\n","\n","Given the lack of labelled data, a very simplistic approach is used to calculate sentiment tone: \n","\n","\\begin{equation*}\n","Net \\ Tone \\  = \\frac{\\#Positive\\ words - \\#Negative\\ words}\n","                    {\\#Positive\\ words + \\#Negative\\ words} \\times \\frac{1}{\\#Total\\ words}\n","\\end{equation*}\n","\n","Where <br>\n","\n","Net tone > 0 points to a positive tone\n","<br>\n","Net tone < 0 points to a negative tone\n","<br>\n","Net tone = 0 points to neutral tone\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZQDpolyTC6jM"},"source":["##  <a name=\"dict\"></a> Dictionary Approach "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sKL38O_HC6jM","colab":{}},"source":["with open(file_path_prefix + 'sentiment/sentiment_pos_dict.pickle', 'rb') as handle:\n","    posDict = pickle.load(handle)\n","    \n","with open(file_path_prefix + 'sentiment/sentiment_neg_dict.pickle', 'rb') as handle:\n","    negDict = pickle.load(handle)\n","\n","\n","corpus_wcount = Counter([item for sublist in tokenized_text for item in sublist])\n","pos_inter = Counter({w:corpus_wcount[w] for w in posDict.intersection(corpus_wcount)})\n","neg_inter = Counter({w:corpus_wcount[w] for w in negDict.intersection(corpus_wcount)})\n","\n","top_pos = pos_inter.most_common(10)\n","top_neg = neg_inter.most_common(10)\n","\n","df_top_pos = pd.DataFrame(top_pos, columns=['token', 'count'])\n","df_top_neg = pd.DataFrame(top_neg, columns=['token', 'count'])\n","\n","fig, ax = plt.subplots(1,2, figsize=(18,8))\n","sns.barplot(x=\"token\", y=\"count\", data=df_top_pos, ax=ax[0])\n","ax[0].set_title('top positive words')\n","sns.barplot(x=\"token\", y=\"count\", data=df_top_neg, label='top negative words', ax=ax[1])\n","ax[1].set_title('top negative words')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"phL_7K2vC6jO"},"source":["These are the top 10 positive and negative words that appear in the corpus. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XFHy-xLvC6jO","colab":{}},"source":["print(\"Number of positive words in corpus: \" +str(sum(pos_inter.values())))\n","print(\"Number of negative words in corpus: \" +str(sum(neg_inter.values())))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xIfcsMofC6jQ"},"source":["## <a name=\"sent-e\"></a> Sentiment Extraction"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vWDCacaiC6jQ","colab":{}},"source":["def RetrieveScore(tokenized_para,positive,negative): \n","    pos_sum = 0\n","    neg_sum = 0\n","    score = 0\n","    if len(tokenized_para) <8:\n","        return 0\n","    for word in tokenized_para: \n","        if word in positive:\n","            pos_sum +=1\n","        elif word in negative:\n","            neg_sum +=1\n","    try:\n","        score = ((pos_sum-neg_sum)/(pos_sum+neg_sum))*(1/len(tokenized_para)) #should this be \n","    except ZeroDivisionError:\n","        score = 0\n","    return score\n","\n","df_paragraphs['sentiment_score'] = df_paragraphs.apply(lambda x: RetrieveScore(x['tokenized_text'],posDict,negDict),axis=1)\n","df_paragraphs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ajrLZjPeuBeO","colab":{}},"source":["df_sentiment = (df_paragraphs['sentiment_score']*df_paragraphs['topic_weights']).apply(pd.Series).rename(columns = topic_dict)\n","df_sentiment = pd.concat([df_sentiment.sum(axis=1).rename('Total'),df_sentiment],axis=1)\n","df_sentiment_m = df_sentiment.groupby(level = [0],sort=False).apply(lambda x: x.sum())\n","df_sentiment_m.plot(figsize=(18,8))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4G4X3N90C6jS"},"source":["The downside of using such a simplistic approach in calculating sentiment is that you can produce some anomalies, in particular for the 'inflation' topic. The word 'inflation' is negative but context is very important when classifying sentiment - in this case the sentence 'Inflation slowed down' should be a positive phrase but is classified as negative. Nevertheless, we can see see a joint dip amongst all topics in 2008, relating to the financial crisis. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZczWIq5TuBeQ","colab":{}},"source":["df_paragraphs.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5kJQYSnmuBeS","colab":{}},"source":["timestamps = df_paragraphs.index.map(lambda x: str(x[0]))\n","paragragh_numbers = df_paragraphs.index.map(lambda x: str(x[1]))\n","#unstacked_idx = [last_day_of_month(pd.to_datetime(i, format=\"%Y%b\")) for i in idx]\n","df_paragraphs['paragragh_number'] = paragragh_numbers\n","df_paragraphs.index = timestamps"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LDoYbsbGuBeU","colab":{}},"source":["df_paragraphs = df_paragraphs.join(df_yc_slope_changes)\n","df_paragraphs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"31Mv3rk2uBeW"},"source":["## <a name=\"rates\"></a> Sentiment Correlations"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A3Lk47uduBeW","colab":{}},"source":["combined_df = df_sentiment_m.join(df_yc_slope_changes.slope_change)\n","combined_df.corr()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Hr2qIXMxuBeY","colab":{}},"source":["print(scipy.stats.pearsonr(combined_df['Total'], combined_df['slope_change']))\n","print(scipy.stats.pearsonr(combined_df['Labor Market'], combined_df['slope_change']))\n","print(scipy.stats.pearsonr(combined_df['Policy'], combined_df['slope_change']))\n","print(scipy.stats.pearsonr(combined_df['Financial Market'], combined_df['slope_change']))\n","print(scipy.stats.pearsonr(combined_df['Inflation'], combined_df['slope_change']))\n","print(scipy.stats.pearsonr(combined_df['Outlook'], combined_df['slope_change']))\n","print(scipy.stats.pearsonr(combined_df['Investment'], combined_df['slope_change']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"f_MCoU5GuBeZ","colab":{}},"source":["fig, ax_left = plt.subplots(figsize=(20,10))\n","ax_right = ax_left.twinx()\n","\n","ax_left.plot(combined_df['Financial Market'], color='blue', label='Financial Market')\n","ax_right.plot(combined_df['slope_change'], color='red', label='yield curve slope change')\n","plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QnJI-rXxuBeb","colab":{}},"source":["plt.figure()\n","f, axes = plt.subplots(2, 1, figsize=(20,10))\n","axes[0].plot(combined_df.index, combined_df['Financial Market'])\n","axes[0].set_ylabel('Financial Market')\n","\n","axes[1].plot(combined_df.index, combined_df['slope_change'])\n","axes[1].set_ylabel('slope_change')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YYVJHAmwC6jS"},"source":["# <a name=\"model\"></a>Modeling"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Bc4zGbSzuBee","colab":{}},"source":["x = df_paragraphs['raw_text']\n","y = df_paragraphs['steepen']\n","\n","indices = list(range(len(y)))\n","minutes_train, minutes_val, is_steepen_train, is_steepen_val, indices_train, indices_val = train_test_split(x, y, indices, test_size=0.3, random_state=7)\n","print(len(minutes_val))\n","print(len(is_steepen_val))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"J9fRxVXMC6jU"},"source":["## <a name=\"base\"></a>Baseline Model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WQrIhmlRuBeg","colab":{}},"source":["pipe = Pipeline([('vectorizer', TfidfVectorizer(tokenizer=simple_tokenizer)), ('svc', LinearSVC())])\n","pipe.fit(minutes_train, is_steepen_train)\n","print(\"Tfidf Train accuracy: \", pipe.score(minutes_train, is_steepen_train))\n","print(\"Tfidf Test accuracy: \", pipe.score(minutes_val, is_steepen_val))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Z1LHstrhuBeh","colab":{}},"source":["pipe = Pipeline([('vectorizer', TfidfVectorizer(tokenizer=simple_tokenizer)), ('topics', NMF(n_components=6, random_state=7)), ('svc', LinearSVC())])\n","pipe.fit(minutes_train, is_steepen_train)\n","print(\"NMF Train accuracy: \", pipe.score(minutes_train, is_steepen_train))\n","print(\"NMF Test accuracy: \", pipe.score(minutes_val, is_steepen_val))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ezgoUPnPuBek","colab":{}},"source":["'''\n","cv = CountVectorizer(tokenizer=simple_tokenizer)\n","count_vectors_train = cv.fit_transform(minutes_train)\n","count_vectors_val = cv.transform(minutes_val)\n","\n","lda = LatentDirichletAllocation(n_components=6)\n","lda_train_vecs = lda.fit_transform(count_vectors_train)\n","lda_test_vecs = lda.transform(count_vectors_val)\n","'''\n","\n","#combine both LDA and sentiment scores\n","lda_sen = np.hstack([np.vstack(df_paragraphs['topic_weights']), np.vstack(df_paragraphs['sentiment_score'])])\n","\n","baseline_train_vecs = lda_sen[indices_train]\n","baseline_val_vecs = lda_sen[indices_val]\n","\n","svc = LinearSVC()\n","svc.fit(baseline_train_vecs, is_steepen_train)\n","train_preds = svc.predict(baseline_train_vecs)\n","test_preds = svc.predict(baseline_val_vecs)\n","\n","print('Baseline Model Train accuracy:', accuracy_score(is_steepen_train, train_preds))\n","print('Baseline Model Test accuracy:', accuracy_score(is_steepen_val, test_preds))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SEJSrxXnC6jp"},"source":["## <a name=\"bert\"></a>GloVe and BERT Embeddings "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"o701cnGZC6jx","colab":{}},"source":["vocab = set([value for valueList in tokenized_text.values for value in valueList ])\n","vocab.add('_UNK')\n","vocab.add('_PAD')\n","\n","print(\"Size of vocab:\", len(vocab))\n","\n","glove_dict = dict()\n","for k in vocab:\n","    glove_dict[k] = nlp(k).vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2SnwHjLjuBep","colab":{}},"source":["glove_vectors = np.empty(shape=(len(df_paragraphs), 300))\n","for row, text in enumerate(df_paragraphs['tokenized_text']):\n","    glove_vectors_arrays = [glove_dict[word] for word in text]\n","    glove_vectors[row] = np.mean(glove_vectors_arrays, axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EyRjyGNbuBeq","colab":{}},"source":["#combine both GloVe and sentiment scores\n","glove_sen = np.hstack([glove_vectors, np.vstack(df_paragraphs['sentiment_score'])])\n","\n","glove_train_vecs = glove_sen[indices_train]\n","glove_val_vecs = glove_sen[indices_val]\n","\n","svc = LinearSVC()\n","svc.fit(glove_train_vecs, is_steepen_train)\n","train_preds = svc.predict(glove_train_vecs)\n","test_preds = svc.predict(glove_val_vecs)\n","\n","print('Baseline Model Train accuracy:', accuracy_score(is_steepen_train, train_preds))\n","print('Baseline Model Test accuracy:', accuracy_score(is_steepen_val, test_preds))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_wptwXvjuBes","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pckMLfmMC6j-","colab":{}},"source":["# basic pretrained model (case-insensitive)\n","MODEL_NAME = 'distilbert-base-uncased'\n","# Load pre-trained model\n","model = DistilBertModel.from_pretrained(MODEL_NAME)\n","# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aotWv3WVC6j_","colab":{}},"source":["samples = np.random.choice(df_paragraphs['raw_text'], size=5)\n","samples.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0a_MeDw9C6kC","colab":{}},"source":["\n","tokens = tokenizer.batch_encode_plus(df_paragraphs['raw_text'],\n","    pad_to_max_length=True, # this implements padding for different length docs\n","    return_tensors=\"pt\", # returning as pytorch tensors\n","    max_length=512,\n","    truncation=True) # this BERT model has a max sequence length of 512 tokens\n","\n","outputs = model(**tokens)\n","outputs[0][:,0].detach().numpy().shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VSYnfS5IC6kE","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}